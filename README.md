# ToxicityDetection_Bias
Machine Learning models are commonly used in online platforms to detect toxicity and ensure a civil and safe environment for citizens. These models are trained on datasets annotated by human raters. However, they tend to be biased toward certain identities leading to unfair application and therefore potentially excluding them from online conversations. This paper reveals that certain identities, in particular LGBTQ+ and black communities, are affected by this unintended bias. It studies the role input data plays in this bias and shows how balancing the training data with respect to discriminated identities and employing specialized rater groups can mitigate the unintended bias without compromising the performance of the model.
